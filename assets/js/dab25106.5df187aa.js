"use strict";(self.webpackChunkdoc_center=self.webpackChunkdoc_center||[]).push([[92954],{17246:(e,t,i)=>{i.r(t),i.d(t,{assets:()=>d,contentTitle:()=>a,default:()=>h,frontMatter:()=>s,metadata:()=>l,toc:()=>o});var r=i(74848),n=i(28453);const s={},a=void 0,l={id:"machine-learning/pytorch/distributed",title:"distributed",description:"TORCH.DISTRIBUTED",source:"@site/docs/01000-machine-learning/pytorch/00200-distributed.md",sourceDirName:"01000-machine-learning/pytorch",slug:"/machine-learning/pytorch/distributed",permalink:"/dddtdd-docs/machine-learning/pytorch/distributed",draft:!1,unlisted:!1,tags:[],version:"current",lastUpdatedBy:"Colin Zuo",lastUpdatedAt:1720187019e3,sidebarPosition:200,frontMatter:{},sidebar:"docSidebar",previous:{title:"install",permalink:"/dddtdd-docs/machine-learning/pytorch/install"},next:{title:"\u642d\u5efaUbuntu\u57fa\u672c\u73af\u5883",permalink:"/dddtdd-docs/os/ubuntu/setup-ubuntu-basic"}},d={},o=[{value:"TORCH.DISTRIBUTED",id:"torchdistributed",level:2},{value:"Which backend to use",id:"which-backend-to-use",level:3},{value:"Initialization",id:"initialization",level:3},{value:"Distributed Key-Value Store",id:"distributed-key-value-store",level:3},{value:"Launch utility",id:"launch-utility",level:3},{value:"PYTORCH DISTRIBUTED OVERVIEW",id:"pytorch-distributed-overview",level:2},{value:"WRITING DISTRIBUTED APPLICATIONS WITH PYTORCH",id:"writing-distributed-applications-with-pytorch",level:3},{value:"DataParallel",id:"dataparallel",level:2},{value:"DATA PARALLELISM",id:"data-parallelism",level:3},{value:"DistributedDataParallel",id:"distributeddataparallel",level:2},{value:"ddp_tutorial",id:"ddp_tutorial",level:3},{value:"torchrun",id:"torchrun",level:2},{value:"Train script",id:"train-script",level:3}];function c(e){const t={a:"a",code:"code",h2:"h2",h3:"h3",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,n.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(t.h2,{id:"torchdistributed",children:"TORCH.DISTRIBUTED"}),"\n",(0,r.jsx)(t.p,{children:(0,r.jsx)(t.a,{href:"https://pytorch.org/docs/1.13/distributed.html",children:"https://pytorch.org/docs/1.13/distributed.html"})}),"\n",(0,r.jsx)(t.h3,{id:"which-backend-to-use",children:"Which backend to use"}),"\n",(0,r.jsxs)(t.ul,{children:["\n",(0,r.jsxs)(t.li,{children:["Use the NCCL backend for distributed ",(0,r.jsx)(t.strong,{children:"GPU training"})]}),"\n",(0,r.jsx)(t.li,{children:"Use the Gloo backend for distributed CPU training"}),"\n"]}),"\n",(0,r.jsx)(t.h3,{id:"initialization",children:"Initialization"}),"\n",(0,r.jsxs)(t.p,{children:["The package needs to be initialized using the torch.distributed.init_process_group() function before calling any other methods. This ",(0,r.jsx)(t.strong,{children:"blocks until all processes have joined"})]}),"\n",(0,r.jsxs)(t.ul,{children:["\n",(0,r.jsxs)(t.li,{children:["\n",(0,r.jsx)(t.p,{children:"MASTER_PORT - required; has to be a free port on machine with rank 0"}),"\n"]}),"\n",(0,r.jsxs)(t.li,{children:["\n",(0,r.jsx)(t.p,{children:"MASTER_ADDR - required (except for rank 0); address of rank 0 node"}),"\n"]}),"\n",(0,r.jsxs)(t.li,{children:["\n",(0,r.jsx)(t.p,{children:"WORLD_SIZE - required; can be set either here, or in a call to init function"}),"\n"]}),"\n",(0,r.jsxs)(t.li,{children:["\n",(0,r.jsx)(t.p,{children:"RANK - required; can be set either here, or in a call to init function"}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(t.p,{children:"The machine with rank 0 will be used to set up all connections"}),"\n",(0,r.jsx)(t.h3,{id:"distributed-key-value-store",children:"Distributed Key-Value Store"}),"\n",(0,r.jsxs)(t.p,{children:["The distributed package comes with a distributed key-value store, which can be used to share information between processes in the group as well as to initialize the distributed package in ",(0,r.jsx)(t.code,{children:"torch.distributed.init_process_group()"})]}),"\n",(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{className:"language-python",children:'import torch.distributed as dist\nfrom datetime import timedelta\n# Run on process 1 (server)\nserver_store = dist.TCPStore("127.0.0.1", 1234, 2, True, timedelta(seconds=30))\n# Run on process 2 (client)\nclient_store = dist.TCPStore("127.0.0.1", 1234, 2, False)\n# Use any of the store methods from either the client or server after initialization\nserver_store.set("first_key", "first_value")\nclient_store.get("first_key")\n'})}),"\n",(0,r.jsx)(t.h3,{id:"launch-utility",children:"Launch utility"}),"\n",(0,r.jsxs)(t.p,{children:["The torch.distributed package also provides a launch utility in ",(0,r.jsx)(t.code,{children:"torch.distributed.launch"}),"\nThis helper utility can be used to launch ",(0,r.jsx)(t.strong,{children:"multiple processes per node"})," for distributed training."]}),"\n",(0,r.jsxs)(t.p,{children:["This module is going to be deprecated in favor of ",(0,r.jsx)(t.code,{children:"torchrun"})]}),"\n",(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{className:"language-bash",children:'python -m torch.distributed.launch --nproc-per-node=NUM_GPUS_YOU_HAVE\n           --nnodes=2 --node-rank=0 --master-addr="192.168.1.1"\n           --master-port=1234 YOUR_TRAINING_SCRIPT.py (--arg1 --arg2 --arg3\n           and all other arguments of your training script)\n'})}),"\n",(0,r.jsx)(t.h2,{id:"pytorch-distributed-overview",children:"PYTORCH DISTRIBUTED OVERVIEW"}),"\n",(0,r.jsxs)(t.p,{children:["\u5728",(0,r.jsx)(t.a,{href:"https://pytorch.org/tutorials/index.html",children:"https://pytorch.org/tutorials/index.html"}),"\u8fd9\u4e2a\u9875\u9762\u9009",(0,r.jsx)(t.strong,{children:"Parallel and Distributed Training"})]}),"\n",(0,r.jsx)(t.p,{children:(0,r.jsx)(t.a,{href:"https://pytorch.org/tutorials/beginner/dist_overview.html",children:"https://pytorch.org/tutorials/beginner/dist_overview.html"})}),"\n",(0,r.jsxs)(t.ul,{children:["\n",(0,r.jsx)(t.li,{children:"Distributed Data-Parallel Training (DDP): With DDP, the model is replicated on every process, and every model replica will be fed with a different set of input data samples"}),"\n",(0,r.jsxs)(t.li,{children:["RPC-Based Distributed Training (RPC): supports general training structures that ",(0,r.jsx)(t.strong,{children:"cannot fit"})," into data-parallel training such as ",(0,r.jsx)(t.strong,{children:"distributed pipeline parallelism"}),", parameter server paradigm, and combinations of DDP with other training paradigms"]}),"\n",(0,r.jsxs)(t.li,{children:["Collective Communication (c10d) library: It offers both ",(0,r.jsx)(t.strong,{children:"collective communication APIs"})," (e.g., all_reduce and all_gather) and ",(0,r.jsx)(t.strong,{children:"P2P communication APIs"})," (e.g., send and isend)"]}),"\n"]}),"\n",(0,r.jsxs)(t.p,{children:["PyTorch provides several options for data-parallel training. For applications that gradually grow ",(0,r.jsx)(t.strong,{children:"from simple to complex and from prototype to production"}),", the common development trajectory would be"]}),"\n",(0,r.jsx)(t.h3,{id:"writing-distributed-applications-with-pytorch",children:"WRITING DISTRIBUTED APPLICATIONS WITH PYTORCH"}),"\n",(0,r.jsx)(t.p,{children:(0,r.jsx)(t.a,{href:"https://pytorch.org/tutorials/intermediate/dist_tuto.html",children:"https://pytorch.org/tutorials/intermediate/dist_tuto.html"})}),"\n",(0,r.jsx)(t.h2,{id:"dataparallel",children:"DataParallel"}),"\n",(0,r.jsx)(t.p,{children:(0,r.jsx)(t.a,{href:"https://pytorch.org/docs/stable/generated/torch.nn.DataParallel.html",children:"https://pytorch.org/docs/stable/generated/torch.nn.DataParallel.html"})}),"\n",(0,r.jsx)(t.p,{children:"This container parallelizes the application of the given module by splitting the input across the specified devices by chunking in the batch dimension (other objects will be copied once per device). In the forward pass, the module is replicated on each device, and each replica handles a portion of the input. During the backwards pass, gradients from each replica are summed into the original module."}),"\n",(0,r.jsx)(t.p,{children:"It is recommended to use DistributedDataParallel, instead of this class, to do multi-GPU training, even if there is only a single node"}),"\n",(0,r.jsx)(t.h3,{id:"data-parallelism",children:"DATA PARALLELISM"}),"\n",(0,r.jsx)(t.p,{children:(0,r.jsx)(t.a,{href:"https://pytorch.org/tutorials/beginner/blitz/data_parallel_tutorial.html",children:"https://pytorch.org/tutorials/beginner/blitz/data_parallel_tutorial.html"})}),"\n",(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{className:"language-python",children:'model = Model(input_size, output_size)\nif torch.cuda.device_count() > 1:\n  print("Let\'s use", torch.cuda.device_count(), "GPUs!")\n  # dim = 0 [30, xxx] -> [10, ...], [10, ...], [10, ...] on 3 GPUs\n  model = nn.DataParallel(model)\n\nmodel.to(device)\n'})}),"\n",(0,r.jsx)(t.h2,{id:"distributeddataparallel",children:"DistributedDataParallel"}),"\n",(0,r.jsxs)(t.p,{children:[(0,r.jsx)(t.a,{href:"https://pytorch.org/docs/1.13/generated/torch.nn.parallel.DistributedDataParallel.html",children:"https://pytorch.org/docs/1.13/generated/torch.nn.parallel.DistributedDataParallel.html"}),"\n",(0,r.jsx)(t.a,{href:"https://pytorch.org/docs/stable/notes/ddp.html",children:"https://pytorch.org/docs/stable/notes/ddp.html"})]}),"\n",(0,r.jsx)(t.p,{children:"This container parallelizes the application of the given module by splitting the input across the specified devices by chunking in the batch dimension. The module is replicated on each machine and each device, and each such replica handles a portion of the input. During the backwards pass, gradients from each node are averaged."}),"\n",(0,r.jsx)(t.h3,{id:"ddp_tutorial",children:"ddp_tutorial"}),"\n",(0,r.jsx)(t.p,{children:(0,r.jsx)(t.a,{href:"https://pytorch.org/tutorials/intermediate/ddp_tutorial.html",children:"https://pytorch.org/tutorials/intermediate/ddp_tutorial.html"})}),"\n",(0,r.jsx)(t.p,{children:"Applications using DDP should spawn multiple processes and create a single DDP instance per process."}),"\n",(0,r.jsx)(t.h2,{id:"torchrun",children:"torchrun"}),"\n",(0,r.jsx)(t.p,{children:(0,r.jsx)(t.a,{href:"https://pytorch.org/docs/stable/elastic/run.html",children:"https://pytorch.org/docs/stable/elastic/run.html"})}),"\n",(0,r.jsx)(t.h3,{id:"train-script",children:"Train script"}),"\n",(0,r.jsx)(t.p,{children:(0,r.jsx)(t.a,{href:"https://pytorch.org/docs/stable/elastic/train_script.html",children:"https://pytorch.org/docs/stable/elastic/train_script.html"})}),"\n",(0,r.jsxs)(t.p,{children:["Make sure you have a ",(0,r.jsx)(t.code,{children:"load_checkpoint(path)"})," and ",(0,r.jsx)(t.code,{children:"save_checkpoint(path)"})," logic in your script. When any number of workers fail we restart all the workers with the same program arguments so you will lose progress up to the most recent checkpoint"]})]})}function h(e={}){const{wrapper:t}={...(0,n.R)(),...e.components};return t?(0,r.jsx)(t,{...e,children:(0,r.jsx)(c,{...e})}):c(e)}},28453:(e,t,i)=>{i.d(t,{R:()=>a,x:()=>l});var r=i(96540);const n={},s=r.createContext(n);function a(e){const t=r.useContext(s);return r.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function l(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(n):e.components||n:a(e.components),r.createElement(s.Provider,{value:t},e.children)}}}]);